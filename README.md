
Этот алгоритм позволит подключаться к бд ClickHouse и вытаскивать большие объемы данных. В данный момент настроен на выгрузку по дням.

## Содержание
* [Технологии](#technologies)
* [Начало работы](#start_working)
  * [Библиотеки](#library)
  * [Описание файлов](#files)
* [Пробный запуск и проверка данных](#try_run)
* [Постобработка выгруженных файлов](#post_processing)
* [Планируемые обновления](#updates)
* [FAQ](#FAQ)
* [Команда проекта](#project_team)


<a name="technologies"><h2>Технологии</h2></a>
* [Official_ClickHouse_site](https://clickhouse.com/docs/en/integrations/python)
* [github_ClickHouse_helper](https://github.com/ClickHouse/clickhouse-connect)



<a name="start_working"><h2>Начало работы</h2></a>

<a name="library"><h3>Библиотеки</h3></a>

Убедитесь, что у вас есть все необходимые библиотеки:
1. Numpy
2. Pandas
3. clickhouse_connect
4. tqdm
5. time

для установки соответвующей библиотеки используйте команды:
1. ```pip install Numpy``` или ```pip3 install Numpy```
2. ```pip install Pandas``` или ```pip3 install Pandas```
3. ```pip install clickhouse-connect``` или ```pip3 install clickhouse-connect```
4. ```pip install tqdm``` или ```pip3 install tqdm```
5. ```pip install time``` или ```pip3 install time```

- при возникновении проблем - ищите ответы на "Stack Overflow"

<a name="files"><h3>Описание файлов</h3></a>
#### 1. "ClickHouse_code.py" - содержит актуальный код для конкретной выгрузке (заменяется для каждой задачи)

```
__QUERY__= """
with
'{date_begin}' as beg_,
'{date_end}' as end___QUERY__= """
with 
'{date_begin}' as beg_,
'{date_end}' as end_
[вставьте здесь Select]
"""
```

#### 2. "ClickHouse_database_configuration.py" - хранит конфигурации для подключения к ClickHouse (один раз заполнили и не трогаем)
```
__HOST__ = 'c-c9qsfmhkl42nof95hnh3.rw.mdb.yandexcloud.net'
__PORT__ = 8443
__CA_CERT__ = ''
__USERNAME__ = ''
__PASSWORD__ = ''
```

__CA_CERT__ - путь до файла дешифровщика RootCA.pem, который вы скачали с [Яндекса](https://yandex.cloud/ru/docs/managed-clickhouse/operations/connect/?utm_referrer=about%3Ablank);
    
__USERNAME__ - ваш LDAP;
    
__PASSWORD__ - пароль, который выдал руководитель (Дарья Голубева).
    
#### 3. "ClickHouse_database.py" - содержит класс с методами для подключения к ClickHouse и сопутствующими обработками (не открываем)
#### 4. "ClickHouse_get_data.py" - основной файл для выгрузки данных, в котором работаем

Это оснвной файл для выгрузки данных. В нем нужно изменить два параметра: временные границы (период; верхняя и нижняя границы включительны) и путь до папки, в который алгоритм будет складывать сформированные файлы. 
Временные границы:
```
day_start = '2024-08-15'
day_end = '2024-09-30'
```
Путь до папки для складирования сформированных файлов:

```csv_file = pathlib.Path(r'Users/user/Desktop/ClickHouse_fetch/fetch_data',"fetch_sql_week_{id}.csv".format(id=num_week))```

   
#### 5. "fetch_data" - папка, в которую сохраняем выгруженные файлы
#### 6. "test.py" - сборка выгруженных файлов
#### 7. "py_log.log" - файл, который генерируется автоматически и содержит дни, в которые возникали ошибки. В будущем дополнится более подробными описаниями возникших ошибок

Необходимо написать путь до папки, в которую алгоритм сложил сформированные файлы:

```df_main = pd.read_csv('/Users/user/Desktop/ClickHouse_fetch/fetch_data/fetch_sql_week_{id}.csv'.format(id=i))```
<a name="try_run"><h2>Пробный запуск и проверка данных</h2></a>

Рекомендую следующий алгоритм действий для быстрой и корректной работы с инструментом:
1. Написать запрос в DBeaver;
2. Убедиться, что он работает в рамках одного дня;
3. Перенести его в файл "ClickHouse_code.py";
4. Перейти в файл "ClickHouse_get_data.py", выставить временные границы выгрузки и путь до папки, куда складывать сформированные файлы;
5. Запустить на пары днях, чтобы убедиться в корректности работы алгоритма;
6. При корректной рабооте запустить на весь период;
7. В файле "test.py" выставить путь до папки, куда алгоритм выгрузил сформированные файлы;
8. Запустиить "test.py", проверить контольные суммы


<a name="post_processing"><h2>Постобработка выгруженных файлов</h2></a>

Пока нет универсальных автотестов, рекомендую после каждой выгрузки делать проверку на контрольные суммы записей/дат и тд


<a name="updates"><h2>Планируемые обновления</h2></a>

1.	Выгрузка не только по дням, но и по неделям/выставленным границам/от объема итерации;
2.	Формирование файлов не только по неделям, а в зависимости от объема одной итерации выгрузки;
3.	Исправление ошибок, при которых останавливается выгрузка:

     *	Ошибка на стороне бд;
     *	Ошибка на стороне запроса;
     *	Ошибка на стороне пользователя;

4.	Более корректное логирование ошибок (реализовано: день – ошибка выгрузки; сделать точное описание ошибки)
5.	Сделать универсальные автотесты на консистентность и полноценность выгрузки
6.	Возможность выгружать не только в csv, но и сразу в google sheets
7.	Вынести пути в отдельную переменную, чтобы все переменные вводились в начале файла
8.	Автоматическое обновление пути к файлам в зависимости от расположения репозитория
9.	Добавить tqdm для отслеживания время отработки кода

**Выставить приоритезацию по обновлениям**


<a name="FAQ"><h2>FAQ</h2></a>
* Здесь будут часто-задаваемые вопросы (добавляйте)

<a name="project_team"><h2>Команда проекта</h2></a>
* [Саитов Владислав](https://portal.vseinstrumenti.ru/profile/6fd3a804-7554-b745-9521-80d154d9b4de)

## Подсказки по написанию readme

ПРАВИЛА ОФОРМЛЕНИЯ ФАЙЛА - [Первая ссылка](https://github.com/sandino/Markdown-Cheatsheet/blob/master/README.md)

Шпаргалка по Markdown - [Вторая ссылка](https://gist.github.com/alinastorm/8a04cdbc36be9c051a66f90ae6d6df35)
